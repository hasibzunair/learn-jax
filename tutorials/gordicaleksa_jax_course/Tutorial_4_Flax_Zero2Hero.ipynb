{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_UZf5Dy2VXu"
      },
      "source": [
        "[<img src=\"https://deepnote.com/buttons/launch-in-deepnote-small.svg\">](https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fgordicaleksa%2Fget-started-with-JAX%2Fblob%2Fmain%2FTutorial_4_Flax_Zero2Hero_Colab.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD9d0-U62VXy"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gordicaleksa/get-started-with-JAX/blob/main/Tutorial_4_Flax_Zero2Hero_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbMr3-5oun69"
      },
      "source": [
        "# Flax: From Zero to Hero!\n",
        "\n",
        "This notebook heavily relies on the [official Flax docs](https://flax.readthedocs.io/en/latest/) and [examples](https://github.com/google/flax/blob/main/examples/)  + some additional code/modifications, comments/notes, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1qve53yeof5"
      },
      "source": [
        "### Enter Flax - the basics ❤️\n",
        "\n",
        "Before you jump into the Flax world I strongly recommend you check out my JAX tutorials, as I won't be covering the details of JAX here.\n",
        "\n",
        "* (Tutorial 1) ML with JAX: From Zero to Hero ([video](https://www.youtube.com/watch?v=SstuvS-tVc0), [notebook](https://github.com/gordicaleksa/get-started-with-JAX/blob/main/Tutorial_1_JAX_Zero2Hero_Colab.ipynb))\n",
        "* (Tutorial 2) ML with JAX: from Hero to Hero Pro+ ([video](https://www.youtube.com/watch?v=CQQaifxuFcs), [notebook](https://github.com/gordicaleksa/get-started-with-JAX/blob/main/Tutorial_2_JAX_HeroPro%2B_Colab.ipynb))\n",
        "* (Tutorial 3) ML with JAX: Coding a Neural Network from Scratch in Pure JAX ([video](https://www.youtube.com/watch?v=6_PqUPxRmjY), [notebook](https://github.com/gordicaleksa/get-started-with-JAX/blob/main/Tutorial_3_JAX_Neural_Network_from_Scratch_Colab.ipynb))\n",
        "\n",
        "That out of the way - let's start with the basics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHcasJkggdZN"
      },
      "outputs": [],
      "source": [
        "# Install Flax and JAX\n",
        "!pip install --upgrade -q \"jax[cuda11_cudnn805]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git\n",
        "!pip install --upgrade -q git+https://github.com/deepmind/dm-haiku  # Haiku is here just for comparison purposes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jaxlib==0.4.2"
      ],
      "metadata": {
        "id": "NN-oXcEJ9Q1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmVx7EjigrEZ"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax import lax, random, numpy as jnp\n",
        "\n",
        "# NN lib built on top of JAX developed by Google Research (Brain team)\n",
        "# Flax was \"designed for flexibility\" hence the name (Flexibility + JAX -> Flax)\n",
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn  # nn notation also used in PyTorch and in Flax's older API\n",
        "from flax.training import train_state  # a useful dataclass to keep train state\n",
        "\n",
        "# DeepMind's NN JAX lib - just for comparison purposes, we're not learning Haiku here\n",
        "import haiku as hk \n",
        "\n",
        "# JAX optimizers - a separate lib developed by DeepMind\n",
        "import optax\n",
        "\n",
        "# Flax doesn't have its own data loading functions - we'll be using PyTorch dataloaders\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Python libs\n",
        "import functools  # useful utilities for functional programs\n",
        "from typing import Any, Callable, Sequence, Optional\n",
        "\n",
        "# Other important 3rd party libs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSDyQLgOesZp"
      },
      "source": [
        "The goal of this notebook is to get you started with Flax!\n",
        "\n",
        "I'll only cover the most essential parts of Flax (and Optax) - just as much as needed to get you started with training NNs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1kdq0P_g7LU"
      },
      "outputs": [],
      "source": [
        "# Let's start with the simplest model possible: a single feed-forward layer (linear regression)\n",
        "model = nn.Dense(features=5)\n",
        "\n",
        "# All of the Flax NN layers inherit from the Module class (similarly to PyTorch)\n",
        "print(nn.Dense.__bases__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux9Okie5PWpw"
      },
      "source": [
        "So how can we do inference with this simple model? 2 steps: init and apply!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QViTvJhFite2"
      },
      "outputs": [],
      "source": [
        "# Step 1: init\n",
        "seed = 23\n",
        "key1, key2 = random.split(random.PRNGKey(seed))\n",
        "x = random.normal(key1, (10,))  # create a dummy input, a 10-dimensional random vector\n",
        "\n",
        "# Initialization call - this gives us the actual model weights \n",
        "# (remember JAX handles state externally!)\n",
        "y, params = model.init_with_output(key2, x)  \n",
        "print(y)\n",
        "print(jax.tree_map(lambda x: x.shape, params))\n",
        "\n",
        "# Note1: automatic shape inference\n",
        "# Note2: immutable structure (hence FrozenDict)\n",
        "# Note3: init_with_output if you care, for whatever reason, about the output here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3yFAqeTjdLj"
      },
      "outputs": [],
      "source": [
        "# Step 2: apply\n",
        "y = model.apply(params, x)  # this is how you run prediction in Flax, state is external!\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31O_mx-Smalq"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    y = model(x)  # this doesn't work anymore (bye bye PyTorch syntax)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQYyv76sCJ25"
      },
      "outputs": [],
      "source": [
        "# todo: a small coding exercise - let's contrast Flax with Haiku\n",
        "\n",
        "model = hk.transform(lambda x: hk.Linear(5)(x))\n",
        "\n",
        "seed = 23\n",
        "key1, key2 = random.split(random.PRNGKey(seed))\n",
        "x = random.normal(key1, (10,))  # create a dummy input, a 10-dimensional random vector\n",
        "\n",
        "params = model.init(key2, x)\n",
        "\n",
        "y = model.apply(params, None, x)\n",
        "print(y)\n",
        "\n",
        "print(hk.Linear.__bases__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UWr3hdpmFBng"
      },
      "outputs": [],
      "source": [
        "#@title Haiku vs Flax solution\n",
        "model = hk.transform(lambda x: hk.Linear(output_size=5)(x))\n",
        "\n",
        "seed = 23\n",
        "key1, key2 = random.split(random.PRNGKey(seed))\n",
        "x = random.normal(key1, (10,))  # create a dummy input, a 10-dimensional random vector\n",
        "\n",
        "params = model.init(key2, x)\n",
        "out = model.apply(params, None, x)\n",
        "print(out)\n",
        "\n",
        "print(hk.Linear.__bases__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWBxTShUiLzW"
      },
      "source": [
        "All of this might (initially!) be overwhelming if you're used to stateful, object-oriented paradigm.\n",
        "\n",
        "What Flax offers is high performance and flexibility (similarly to JAX).\n",
        "\n",
        "Here are some [benchmark numbers](https://github.com/huggingface/transformers/tree/master/examples/flax/text-classification) from the HuggingFace team.\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUBYtd40krx1"
      },
      "source": [
        "Now that we have a an answer to \"why should I learn Flax?\" - let's start our descent into Flaxlandia!\n",
        "\n",
        "### A toy example 🚚 - training a linear regression model\n",
        "\n",
        "We'll first implement a pure-JAX appoach and then we'll do it the Flax-way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53-TXcbYkt9D"
      },
      "outputs": [],
      "source": [
        "# Defining a toy dataset\n",
        "n_samples = 1500\n",
        "x_dim = 2  # putting small numbers here so that we can visualize the data easily\n",
        "y_dim = 1\n",
        "noise_amplitude = 0.1\n",
        "\n",
        "# Generate (random) ground truth W and b\n",
        "# Note: we could get W, b from a randomely initialized nn.Dense here, being closer to JAX for now \n",
        "key, w_key, b_key = random.split(random.PRNGKey(seed), num=3)\n",
        "W = random.normal(w_key, (x_dim, y_dim))  # weight\n",
        "b = random.normal(b_key, (y_dim,))  # bias\n",
        "\n",
        "# This is the structure that Flax expects (recall from the previous section!)\n",
        "true_params = freeze({'params': {'bias': b, 'kernel': W}})\n",
        "\n",
        "# Generate samples with additional noise\n",
        "key, x_key, noise_key = random.split(key, num=3)\n",
        "xs = random.normal(x_key, (n_samples, x_dim))\n",
        "ys = jnp.dot(xs, W) + b\n",
        "ys += noise_amplitude * random.normal(noise_key, (n_samples, y_dim))\n",
        "print(f'xs shape = {xs.shape} ; ys shape = {ys.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc4-xoIapKCs"
      },
      "outputs": [],
      "source": [
        "# Let's visualize our data (becoming one with the data paradigm <3)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "assert xs.shape[-1] == 2 and ys.shape[-1] == 1  # low dimensional data so that we can plot it\n",
        "ax.scatter(xs[:, 0], xs[:, 1], zs=ys)\n",
        "\n",
        "# todo: exercise - let's show that our data lies on the 2D plane embedded in 3D\n",
        "# option 1: analytic approach\n",
        "# option 2: data-driven approach\n",
        "\n",
        "# y = w*x + b -> w1*x1 + w2*x2 + b -> 2D plane in 3D space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKiCOyoikxcM"
      },
      "outputs": [],
      "source": [
        "def make_mse_loss(xs, ys):\n",
        "    \n",
        "    def mse_loss(params):\n",
        "        \"\"\"Gives the value of the loss on the (xs, ys) dataset for the given model (params).\"\"\"\n",
        "        \n",
        "        # Define the squared loss for a single pair (x,y)\n",
        "        def squared_error(x, y):\n",
        "            pred = model.apply(params, x)\n",
        "            # Inner because 'y' could have in general more than 1 dims\n",
        "            return jnp.inner(y-pred, y-pred) / 2.0\n",
        "\n",
        "        # Batched version via vmap\n",
        "        return jnp.mean(jax.vmap(squared_error)(xs, ys), axis=0)\n",
        "\n",
        "    return jax.jit(mse_loss)  # and finally we jit the result (mse_loss is a pure function)\n",
        "\n",
        "mse_loss = make_mse_loss(xs, ys)\n",
        "value_and_grad_fn = jax.value_and_grad(mse_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phLYjH5ZkzLn"
      },
      "outputs": [],
      "source": [
        "# Let's reuse the simple feed-forward layer since it trivially implements linear regression\n",
        "model = nn.Dense(features=y_dim)\n",
        "params = model.init(key, xs)\n",
        "print(f'Initial params = {params}')\n",
        "\n",
        "# Let's set some reasonable hyperparams\n",
        "lr = 0.3\n",
        "epochs = 20\n",
        "log_period_epoch = 5\n",
        "\n",
        "print('-' * 50)\n",
        "for epoch in range(epochs):\n",
        "    loss, grads = value_and_grad_fn(params)\n",
        "    # SGD (closer to JAX again, but we'll progressively go towards how stuff is done in Flax)\n",
        "    params = jax.tree_map(lambda p, g: p - lr * g, params, grads)\n",
        "\n",
        "    if epoch % log_period_epoch == 0:\n",
        "        print(f'epoch {epoch}, loss = {loss}')\n",
        "\n",
        "print('-' * 50)\n",
        "print(f'Learned params = {params}')\n",
        "print(f'Gt params = {true_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvy6Oow2lLHu"
      },
      "source": [
        "Now let's do the same thing but this time with dedicated optimizers!\n",
        "\n",
        "Enter DeepMind's optax! ❤️🔥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hhcFZ7UlCov"
      },
      "outputs": [],
      "source": [
        "opt_sgd = optax.sgd(learning_rate=lr)\n",
        "opt_state = opt_sgd.init(params)  # always the same pattern - handling state externally\n",
        "print(opt_state)\n",
        "# todo: exercise - compare Adam's and SGD's states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_EHHjy_lFGN"
      },
      "outputs": [],
      "source": [
        "params = model.init(key, xs)  # let's start with fresh params again\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss, grads = value_and_grad_fn(params)\n",
        "    updates, opt_state = opt_sgd.update(grads, opt_state)  # arbitrary optim logic!\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    if epoch % log_period_epoch == 0:\n",
        "        print(f'epoch {epoch}, loss = {loss}')\n",
        "\n",
        "# Note 1: as expected we get the same loss values\n",
        "# Note 2: we'll later see more concise ways to handle all of these state components (hint: TrainState)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF1gAYSzxQ1R"
      },
      "source": [
        "In this toy SGD example Optax may not seem that useful but it's very powerful.\n",
        "\n",
        "You can build arbitrary optimizers with arbitrary hyperparam schedules, chaining, param freezing, etc. You can check the [official docs here](https://optax.readthedocs.io/en/latest/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rKbis5O0KQYH"
      },
      "outputs": [],
      "source": [
        "#@title Optax Advanced Examples\n",
        "# This cell won't \"compile\" (no ml_collections package) and serves just as an example\n",
        "\n",
        "# Example from Flax (ImageNet example)\n",
        "# https://github.com/google/flax/blob/main/examples/imagenet/train.py#L88\n",
        "def create_learning_rate_fn(\n",
        "    config: ml_collections.ConfigDict,\n",
        "    base_learning_rate: float,\n",
        "    steps_per_epoch: int):\n",
        "  \"\"\"Create learning rate schedule.\"\"\"\n",
        "  warmup_fn = optax.linear_schedule(\n",
        "      init_value=0., end_value=base_learning_rate,\n",
        "      transition_steps=config.warmup_epochs * steps_per_epoch)\n",
        "  cosine_epochs = max(config.num_epochs - config.warmup_epochs, 1)\n",
        "  cosine_fn = optax.cosine_decay_schedule(\n",
        "      init_value=base_learning_rate,\n",
        "      decay_steps=cosine_epochs * steps_per_epoch)\n",
        "  schedule_fn = optax.join_schedules(\n",
        "      schedules=[warmup_fn, cosine_fn],\n",
        "      boundaries=[config.warmup_epochs * steps_per_epoch])\n",
        "  return schedule_fn\n",
        "\n",
        "tx = optax.sgd(\n",
        "      learning_rate=learning_rate_fn,\n",
        "      momentum=config.momentum,\n",
        "      nesterov=True,\n",
        ")\n",
        "\n",
        "# Example from Haiku (ImageNet example)\n",
        "# https://github.com/deepmind/dm-haiku/blob/main/examples/imagenet/train.py#L116\n",
        "def make_optimizer() -> optax.GradientTransformation:\n",
        "  \"\"\"SGD with nesterov momentum and a custom lr schedule.\"\"\"\n",
        "  return optax.chain(\n",
        "      optax.trace(\n",
        "          decay=FLAGS.optimizer_momentum,\n",
        "          nesterov=FLAGS.optimizer_use_nesterov),\n",
        "      optax.scale_by_schedule(lr_schedule), optax.scale(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFAeHIEwL0ZH"
      },
      "source": [
        "Now let's go beyond these extremely simple models!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_33y-bTl6bd"
      },
      "source": [
        "### Creating custom models ⭐"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOrJHqTSl75M"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    num_neurons_per_layer: Sequence[int]  # data field (nn.Module is Python's dataclass)\n",
        "\n",
        "    def setup(self):  # because dataclass is implicitly using the  __init__ function... :')\n",
        "        self.layers = [nn.Dense(n) for n in self.num_neurons_per_layer]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        activation = x\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            activation = layer(activation)\n",
        "            if i != len(self.layers) - 1:\n",
        "                activation = nn.relu(activation)\n",
        "        return activation\n",
        "\n",
        "x_key, init_key = random.split(random.PRNGKey(seed))\n",
        "\n",
        "model = MLP(num_neurons_per_layer=[16, 8, 1])  # define an MLP model\n",
        "x = random.uniform(x_key, (4,4))  # dummy input\n",
        "params = model.init(init_key, x)  # initialize via init\n",
        "y = model.apply(params, x)  # do a forward pass via apply\n",
        "\n",
        "print(jax.tree_map(jnp.shape, params))\n",
        "print(f'Output: {y}')\n",
        "\n",
        "# todo: exercise - use @nn.compact pattern instead\n",
        "# todo: check out https://realpython.com/python-data-classes/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEhC-WdPnAYp"
      },
      "source": [
        "Great! \n",
        "\n",
        "Now that we know how to build more complex models let's dive deeper and understand how the 'nn.Dense' module is designed itself.\n",
        "\n",
        "#### Introducing \"param\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9YhSgxjnBQg"
      },
      "outputs": [],
      "source": [
        "class MyDenseImp(nn.Module):\n",
        "    num_neurons: int\n",
        "    weight_init: Callable = nn.initializers.lecun_normal()\n",
        "    bias_init: Callable = nn.initializers.zeros\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        weight = self.param('weight',  # parametar name (as it will appear in the FrozenDict)\n",
        "                self.weight_init,  # initialization function, RNG passed implicitly through init fn\n",
        "                (x.shape[-1], self.num_neurons))  # shape info\n",
        "        bias = self.param('bias', self.bias_init, (self.num_neurons,))\n",
        "\n",
        "        return jnp.dot(x, weight)  + bias\n",
        "\n",
        "x_key, init_key = random.split(random.PRNGKey(seed))\n",
        "\n",
        "model = MyDenseImp(num_neurons=3)  # initialize the model\n",
        "x = random.uniform(x_key, (4,4))  # dummy input\n",
        "params = model.init(init_key, x)  # initialize via init\n",
        "y = model.apply(params, x)  # do a forward pass via apply\n",
        "\n",
        "print(jax.tree_map(jnp.shape, params))\n",
        "print(f'Output: {y}')\n",
        "\n",
        "# todo: exercise - check out the source code:\n",
        "# https://github.com/google/flax/blob/main/flax/linen/linear.py\n",
        "# https://github.com/google/jax/blob/main/jax/_src/nn/initializers.py#L150 <- to see why lecun_normal() vs zeros (no brackets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqCPhl9fBI_Z"
      },
      "outputs": [],
      "source": [
        "from inspect import signature\n",
        "\n",
        "# You can see it expects a PRNG key and it is passed implicitly through the init fn (same for zeros)\n",
        "print(signature(nn.initializers.lecun_normal()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWB8HvLHn6g0"
      },
      "source": [
        "So far we've only seen **trainable** params. \n",
        "\n",
        "ML models often times have variables which are part of the state but are not optimized via gradient descent.\n",
        "\n",
        "Let's see how we can handle them using a simple (and contrived) example!\n",
        "\n",
        "#### Introducing \"variable\"\n",
        "\n",
        "*Note on terminology: variable is a broader term and it includes both params (trainable variables) as well as non-trainable vars.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGE6qTHHngYh"
      },
      "outputs": [],
      "source": [
        "class BiasAdderWithRunningMean(nn.Module):\n",
        "    decay: float = 0.99\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        is_initialized = self.has_variable('batch_stats', 'ema')\n",
        "\n",
        "        # 'batch_stats' is not an arbitrary name!\n",
        "        # Flax uses that name in its implementation of BatchNorm (hard-coded, probably not the best of designs?)\n",
        "        ema = self.variable('batch_stats', 'ema', lambda shape: jnp.zeros(shape), x.shape[1:])\n",
        "\n",
        "        # self.param will by default add this variable to 'params' collection (vs 'batch_stats' above)\n",
        "        # Again some idiosyncrasies here we need to pass a key even though we don't actually use it...\n",
        "        bias = self.param('bias', lambda key, shape: jnp.zeros(shape), x.shape[1:])\n",
        "\n",
        "        if is_initialized:\n",
        "            # self.variable returns a reference hence .value\n",
        "            ema.value = self.decay * ema.value + (1.0 - self.decay) * jnp.mean(x, axis=0, keepdims=True)\n",
        "\n",
        "        return x - ema.value + bias\n",
        "\n",
        "x_key, init_key = random.split(random.PRNGKey(seed))\n",
        "\n",
        "model = BiasAdderWithRunningMean()\n",
        "x = random.uniform(x_key, (10,4))  # dummy input\n",
        "variables = model.init(init_key, x)\n",
        "print(f'Multiple collections = {variables}')  # we can now see a new collection 'batch_stats'\n",
        "\n",
        "# We have to use mutable since regular params are not modified during the forward\n",
        "# pass, but these variables are. We can't keep state internally (because JAX) so we have to return it.\n",
        "y, updated_non_trainable_params = model.apply(variables, x, mutable=['batch_stats'])\n",
        "print(updated_non_trainable_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuzwVt8RoHvY"
      },
      "outputs": [],
      "source": [
        "# Let's see how we could train such model!\n",
        "def update_step(opt, apply_fn, x, opt_state, params, non_trainable_params):\n",
        "\n",
        "    def loss_fn(params):\n",
        "        y, updated_non_trainable_params = apply_fn(\n",
        "            {'params': params, **non_trainable_params}, \n",
        "            x, mutable=list(non_trainable_params.keys()))\n",
        "        \n",
        "        loss = ((x - y) ** 2).sum()  # not doing anything really, just for the demo purpose\n",
        "\n",
        "        return loss, updated_non_trainable_params\n",
        "\n",
        "    (loss, non_trainable_params), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    \n",
        "    return opt_state, params, non_trainable_params  # all of these represent the state - ugly, for now\n",
        "\n",
        "model = BiasAdderWithRunningMean()\n",
        "x = jnp.ones((10,4))  # dummy input, using ones because it's easier to see what's going on\n",
        "\n",
        "variables = model.init(random.PRNGKey(seed), x)\n",
        "non_trainable_params, params = variables.pop('params')\n",
        "del variables  # delete variables to avoid wasting resources (this pattern is used in the official code)\n",
        "\n",
        "sgd_opt = optax.sgd(learning_rate=0.1)  # originally you'll see them use the 'tx' naming (from opTaX)\n",
        "opt_state = sgd_opt.init(params)\n",
        "\n",
        "for _ in range(3):\n",
        "    # We'll later see how TrainState abstraction will make this step much more elegant!\n",
        "    opt_state, params, non_trainable_params = update_step(sgd_opt, model.apply, x, opt_state, params, non_trainable_params)\n",
        "    print(non_trainable_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzWUq5vBrWMe"
      },
      "source": [
        "Let's go a level up in abstraction again now that we understand params and variables!\n",
        "\n",
        "Certain layers like BatchNorm will use variables in the background.\n",
        "\n",
        "Let's see a last example that is conceptually as complicated as it gets when it comes to Flax's idiosyncrasies, and high-level at the same time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDw2986orY0a"
      },
      "outputs": [],
      "source": [
        "class DDNBlock(nn.Module):\n",
        "    \"\"\"Dense, dropout + batchnorm combo.\n",
        "\n",
        "    Contains trainable variables (params), non-trainable variables (batch stats),\n",
        "    and stochasticity in the forward pass (because of dropout).\n",
        "    \"\"\"\n",
        "    num_neurons: int\n",
        "    training: bool\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(self.num_neurons)(x)\n",
        "        x = nn.Dropout(rate=0.5, deterministic=not self.training)(x)\n",
        "        x = nn.BatchNorm(use_running_average=not self.training)(x)\n",
        "        return x\n",
        "\n",
        "key1, key2, key3, key4 = random.split(random.PRNGKey(seed), 4)\n",
        "\n",
        "model = DDNBlock(num_neurons=3, training=True)\n",
        "x = random.uniform(key1, (3,4,4))\n",
        "\n",
        "# New: because of Dropout we now have to include its unique key - kinda weird, but you get used to it\n",
        "variables = model.init({'params': key2, 'dropout': key3}, x)\n",
        "print(variables)\n",
        "\n",
        "# And same here, everything else remains the same as the previous example\n",
        "y, non_trainable_params = model.apply(variables, x, rngs={'dropout': key4}, mutable=['batch_stats'])\n",
        "\n",
        "# Let's run these model variables during \"evaluation\":\n",
        "eval_model = DDNBlock(num_neurons=3, training=False)\n",
        "# Because training=False we don't have stochasticity in the forward pass neither do we update the stats\n",
        "y = eval_model.apply(variables, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys1y-yM8vzT8"
      },
      "source": [
        "### A fully-fledged CNN on MNIST example in Flax! 💥\n",
        "\n",
        "Modified the official MNIST example here: https://github.com/google/flax/tree/main/examples/mnist\n",
        "\n",
        "We'll be using PyTorch dataloading instead of TFDS.\n",
        "\n",
        "Let's start by defining a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD8t9K2Nv0yC"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):  # lots of hardcoding, but it serves a purpose for a simple demo\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "        x = x.reshape((x.shape[0], -1))  # flatten\n",
        "        x = nn.Dense(features=256)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(features=10)(x)\n",
        "        x = nn.log_softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVgWLMhiSAYv"
      },
      "source": [
        "Let's add the data loading support in PyTorch!\n",
        "\n",
        "I'll be reusing code from [tutorial #3](https://github.com/gordicaleksa/get-started-with-JAX/blob/main/Tutorial_3_JAX_Neural_Network_from_Scratch_Colab.ipynb):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ-og2UOUUWD"
      },
      "outputs": [],
      "source": [
        "def custom_transform(x):\n",
        "    # A couple of modifications here compared to tutorial #3 since we're using a CNN\n",
        "    # Input: (28, 28) uint8 [0, 255] torch.Tensor, Output: (28, 28, 1) float32 [0, 1] np array\n",
        "    return np.expand_dims(np.array(x, dtype=np.float32), axis=2) / 255.\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Provides us with batches of numpy arrays and not PyTorch's tensors.\"\"\"\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    labels = np.array(transposed_data[1])\n",
        "    imgs = np.stack(transposed_data[0])\n",
        "\n",
        "    return imgs, labels\n",
        "\n",
        "mnist_img_size = (28, 28, 1)\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = MNIST(root='train_mnist', train=True, download=True, transform=custom_transform)\n",
        "test_dataset = MNIST(root='test_mnist', train=False, download=True, transform=custom_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n",
        "\n",
        "# optimization - loading the whole dataset into memory\n",
        "train_images = jnp.array(train_dataset.data)\n",
        "train_lbls = jnp.array(train_dataset.targets)\n",
        "\n",
        "# np.expand_dims is to convert shape from (10000, 28, 28) -> (10000, 28, 28, 1)\n",
        "# We don't have to do this for training images because custom_transform does it for us.\n",
        "test_images = np.expand_dims(jnp.array(test_dataset.data), axis=3)\n",
        "test_lbls = jnp.array(test_dataset.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HeXX51NU0k6"
      },
      "outputs": [],
      "source": [
        "# Visualize a single image\n",
        "imgs, lbls = next(iter(test_loader))\n",
        "img = imgs[0].reshape(mnist_img_size)[:, :, 0]\n",
        "gt_lbl = lbls[0]\n",
        "\n",
        "print(gt_lbl)\n",
        "plt.imshow(img); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsGPQKx0SPL-"
      },
      "source": [
        "Great - we have our data pipeline ready and the model architecture defined.\n",
        "\n",
        "Now let's define core training functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD8ztbEsVM43"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(state, imgs, gt_labels):\n",
        "    def loss_fn(params):\n",
        "        logits = CNN().apply({'params': params}, imgs)\n",
        "        one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=10)\n",
        "        loss = -jnp.mean(jnp.sum(one_hot_gt_labels * logits, axis=-1))\n",
        "        return loss, logits\n",
        "  \n",
        "    (_, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "    state = state.apply_gradients(grads=grads)  # this is the whole update now! concise!\n",
        "    metrics = compute_metrics(logits=logits, gt_labels=gt_labels)  # duplicating loss calculation but it's a bit cleaner\n",
        "    return state, metrics\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(state, imgs, gt_labels):\n",
        "    logits = CNN().apply({'params': state.params}, imgs)\n",
        "    return compute_metrics(logits=logits, gt_labels=gt_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5VblVs2VWxo"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(state, dataloader, epoch):\n",
        "    \"\"\"Train for 1 epoch on the training set.\"\"\"\n",
        "    batch_metrics = []\n",
        "    for cnt, (imgs, labels) in enumerate(dataloader):\n",
        "        state, metrics = train_step(state, imgs, labels)\n",
        "        batch_metrics.append(metrics)\n",
        "\n",
        "    # Aggregate the metrics\n",
        "    batch_metrics_np = jax.device_get(batch_metrics)  # pull from the accelerator onto host (CPU)\n",
        "    epoch_metrics_np = {\n",
        "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
        "        for k in batch_metrics_np[0]\n",
        "    }\n",
        "\n",
        "    return state, epoch_metrics_np\n",
        "\n",
        "def evaluate_model(state, test_imgs, test_lbls):\n",
        "    \"\"\"Evaluate on the validation set.\"\"\"\n",
        "    metrics = eval_step(state, test_imgs, test_lbls)\n",
        "    metrics = jax.device_get(metrics)  # pull from the accelerator onto host (CPU)\n",
        "    metrics = jax.tree_map(lambda x: x.item(), metrics)  # np.ndarray -> scalar\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiV5yiA4BKEk"
      },
      "outputs": [],
      "source": [
        "# This one will keep things nice and tidy compared to our previous examples\n",
        "def create_train_state(key, learning_rate, momentum):\n",
        "    cnn = CNN()\n",
        "    params = cnn.init(key, jnp.ones([1, *mnist_img_size]))['params']\n",
        "    sgd_opt = optax.sgd(learning_rate, momentum)\n",
        "    # TrainState is a simple built-in wrapper class that makes things a bit cleaner\n",
        "    return train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=sgd_opt)\n",
        "\n",
        "def compute_metrics(*, logits, gt_labels):\n",
        "    one_hot_gt_labels = jax.nn.one_hot(gt_labels, num_classes=10)\n",
        "\n",
        "    loss = -jnp.mean(jnp.sum(one_hot_gt_labels * logits, axis=-1))\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == gt_labels)\n",
        "\n",
        "    metrics = {\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy,\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8EFriHnVcJO"
      },
      "outputs": [],
      "source": [
        "# Finally let's define the high-level training/val loops\n",
        "seed = 0  # needless to say these should be in a config or defined like flags\n",
        "learning_rate = 0.1\n",
        "momentum = 0.9\n",
        "num_epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "train_state = create_train_state(jax.random.PRNGKey(seed), learning_rate, momentum)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_state, train_metrics = train_one_epoch(train_state, train_loader, epoch)\n",
        "    print(f\"Train epoch: {epoch}, loss: {train_metrics['loss']}, accuracy: {train_metrics['accuracy'] * 100}\")\n",
        "\n",
        "    test_metrics = evaluate_model(train_state, test_images, test_lbls)\n",
        "    print(f\"Test epoch: {epoch}, loss: {test_metrics['loss']}, accuracy: {test_metrics['accuracy'] * 100}\")\n",
        "\n",
        "# todo: exercise - how would we go about adding dropout? What about BatchNorm? What would have to change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U-BIjQ1v4ff"
      },
      "source": [
        "Bonus point: a walk-through the \"non-toy\", distributed ImageNet CNN training example.\n",
        "\n",
        "Head over to https://github.com/google/flax/tree/main/examples/imagenet\n",
        "\n",
        "You'll keep seeing the same pattern/structure in all official Flax examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q4C2M2tv_0J"
      },
      "source": [
        "### Further learning resources 📚\n",
        "\n",
        "Aside from the [official docs](https://flax.readthedocs.io/en/latest/) and [examples](https://github.com/google/flax/tree/main/examples) I found [HuggingFace's Flax examples](https://github.com/huggingface/transformers/tree/master/examples/flax) and the resources from their [\"community week\"](https://github.com/huggingface/transformers/tree/master/examples/research_projects/jax-projects) useful as well.\n",
        "\n",
        "Finally, [source code](https://github.com/google/flax) is also your friend, as the library is still evolving."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5DqxlZ-SD3e"
      },
      "source": [
        "### Connect with me ❤️\n",
        "\n",
        "Last but not least I regularly post AI-related stuff (paper summaries, AI news, etc.) on my Twitter/LinkedIn. We also have an ever increasing Discord community (1600+ members at the time of writing this). If you care about any of these I encourage you to connect! \n",
        "\n",
        "Social: <br/>\n",
        "💼 LinkedIn - https://www.linkedin.com/in/aleksagordic/ <br/>\n",
        "🐦 Twitter - https://twitter.com/gordic_aleksa <br/>\n",
        "👨‍👩‍👧‍👦 Discord - https://discord.gg/peBrCpheKE <br/>\n",
        "🙏 Patreon - https://www.patreon.com/theaiepiphany <br/>\n",
        "\n",
        "Content: <br/>\n",
        "📺 YouTube - https://www.youtube.com/c/TheAIEpiphany/ <br/>\n",
        "📚 Medium - https://gordicaleksa.medium.com/ <br/>\n",
        "💻 GitHub - https://github.com/gordicaleksa <br/>\n",
        "📢 AI Newsletter - https://aiepiphany.substack.com/ <br/>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tutorial 4: Flax Zero2Hero.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}